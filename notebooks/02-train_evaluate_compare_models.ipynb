{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, Evaluating and Comparing Models\n",
    "\n",
    "In this notebook, different model architectures will be trained on the main data\n",
    "as well as on the integrated data. All models will be evaluated, and finally\n",
    "compared to another to select the best performing one.\n",
    "\n",
    "Here, the train, val and test data is just loaded from files.\n",
    "The data analysis and preparation was done in the notebook\n",
    "`01-data_preparation.ipynb`.\n",
    "\n",
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# paths\n",
    "PATH_DATA = Path(\"../data/processed/main/\")\n",
    "PATH_MODELS = Path(\"../models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "# train data\n",
    "X_train = pd.read_csv(PATH_DATA / \"sleep_data_main_train_features.csv\")\n",
    "y_train = pd.read_csv(PATH_DATA / \"sleep_data_main_train_labels.csv\")\n",
    "\n",
    "# validation data\n",
    "X_val = pd.read_csv(PATH_DATA / \"sleep_data_main_val_features.csv\")\n",
    "y_val = pd.read_csv(PATH_DATA / \"sleep_data_main_val_labels.csv\")\n",
    "\n",
    "# test data\n",
    "X_test = pd.read_csv(PATH_DATA / \"sleep_data_main_test_features.csv\")\n",
    "y_test = pd.read_csv(PATH_DATA / \"sleep_data_main_test_labels.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME\n",
    "# this function works for sklearn models so far\n",
    "# it must work for XGBoost as well\n",
    "# it may be that it works for XGBoost alrready, but I didn't test that so far\n",
    "\n",
    "# function for trianing any regression model and evaluating it using RMSE as metric\n",
    "def train_evaluate_model_sklearn(model,\n",
    "                                  X_t=X_train,\n",
    "                                  y_t=y_train,\n",
    "                                  X_v=X_val,\n",
    "                                  y_v=y_val):\n",
    "    \"\"\"\n",
    "    Train a model and evaluate it using RMSE as metric.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: sklearn model\n",
    "        The model to train.\n",
    "        Model hyperparameters must be set beforehand by either initializing a\n",
    "        model object with the desired parameters and passing it here, or by\n",
    "        directly setting them in the function call.\n",
    "    X_t: pd.DataFrame\n",
    "        The training features.\n",
    "        Default: X_train\n",
    "    y_t: pd.Series\n",
    "        The training labels.\n",
    "        Default: y_train\n",
    "    X_v: pd.DataFrame\n",
    "        The validation features.\n",
    "        Default: X_val\n",
    "    y_v: pd.Series\n",
    "        The validation labels.\n",
    "        Default: y_val\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    rmse: float\n",
    "        The RMSE of the model on the validation set rounded to 4 decimal places.\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert y to 1d array\n",
    "    y_t = y_t.squeeze()\n",
    "    y_v = y_v.squeeze()\n",
    "\n",
    "    model.fit(X_t, y_t)\n",
    "    y_p = model.predict(X_v)\n",
    "    rmse = root_mean_squared_error(y_v, y_p)\n",
    "    rmse = round(rmse, 4)\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "\n",
    "\n",
    "# function for trianing any regression model and evaluating it using RMSE as metric\n",
    "# works for any model used here\n",
    "def train_evaluate_model(model,\n",
    "                         framework,\n",
    "                         hyperparameters=None,\n",
    "                         X_t=X_train,\n",
    "                         y_t=y_train,\n",
    "                         X_v=X_val,\n",
    "                         y_v=y_val):\n",
    "    \"\"\"\n",
    "    Train a model and evaluate it using RMSE as metric.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: sklearn model or xgboost model\n",
    "        The model to train.\n",
    "        Model hyperparameters must be set beforehand by either initializing a\n",
    "        model object with the desired parameters and passing it here, or by\n",
    "        directly setting them in the function call.\n",
    "    framework: str\n",
    "        The framework used to train the model.\n",
    "        Must be either \"sklearn\" or \"xgboost\".\n",
    "    hyperparameters: dict\n",
    "        The hyperparameters of the model.\n",
    "        Default: None, so models are trained with default hyperparameters when\n",
    "        no hyperparameters are passed.\n",
    "    X_t: pd.DataFrame\n",
    "        The training features.\n",
    "        Default: X_train\n",
    "    y_t: pd.Series\n",
    "        The training labels.\n",
    "        Default: y_train\n",
    "    X_v: pd.DataFrame\n",
    "        The validation features.\n",
    "        Default: X_val\n",
    "    y_v: pd.Series\n",
    "        The validation labels.\n",
    "        Default: y_val\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    rmse: float\n",
    "        The RMSE of the model on the validation set rounded to 4 decimal places.\n",
    "    \"\"\"\n",
    "    \n",
    "    # sklearn model\n",
    "    if framework == \"sklearn\":\n",
    "        \n",
    "        # convert y to 1d array\n",
    "        y_t = y_t.squeeze()\n",
    "        y_v = y_v.squeeze()\n",
    "\n",
    "        # set hyperparameters if passed\n",
    "        if hyperparameters is not None:\n",
    "            model.set_params(**hyperparameters)\n",
    "\n",
    "        model.fit(X_t, y_t)\n",
    "        y_p = model.predict(X_v)\n",
    "        rmse = root_mean_squared_error(y_v, y_p)\n",
    "        rmse = round(rmse, 4)\n",
    "    \n",
    "    # xgboost model\n",
    "    #elif framework == \"xgboost\":\n",
    "    \n",
    "    # return rmse regardless of framework used\n",
    "    return rmse\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve it using Scikit-Learn's built in function for grid seach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      "Model: linear_regression\n",
      "Best parameters: {'fit_intercept': True, 'positive': True}\n",
      "Best RMSE: 0.3393\n",
      "\n",
      "Model: random_forest\n",
      "Best parameters: {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Best RMSE: 0.3424\n",
      "\n",
      "Model: xgboost\n",
      "Best parameters: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}\n",
      "Best RMSE: 0.3376\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# create dataframe with models and their parameter grids\n",
    "models_df = pd.DataFrame([\n",
    "    {\n",
    "        'name': 'linear_regression',\n",
    "        'model': LinearRegression(),\n",
    "        'param_grid': {\n",
    "            'fit_intercept': [True, False],\n",
    "            'positive': [True, False]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'random_forest',\n",
    "        'model': RandomForestRegressor(random_state=1337, n_jobs=-1),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'xgboost',\n",
    "        'model': xgb.XGBRegressor(random_state=1337, n_jobs=-1),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [3, 6, 9],\n",
    "            'learning_rate': [0.01, 0.1, 0.3]\n",
    "        }\n",
    "    }\n",
    "])\n",
    "\n",
    "def train_model_with_gridsearch(row):\n",
    "    \"\"\"Train model using GridSearchCV and return best results.\"\"\"\n",
    "    # setup grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=row['model'],\n",
    "        param_grid=row['param_grid'],\n",
    "        scoring='neg_root_mean_squared_error',  # sklearn uses negative RMSE\n",
    "        cv=5,                                   # 5-fold cross-validation\n",
    "        n_jobs=-1,                             # use all cores\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # fit grid search\n",
    "    grid_search.fit(X_train, y_train.squeeze())\n",
    "    \n",
    "    # get best results\n",
    "    best_rmse = -grid_search.best_score_  # convert back to positive RMSE\n",
    "    \n",
    "    return pd.Series({\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_rmse': round(best_rmse, 4),\n",
    "        'best_model': grid_search.best_estimator_\n",
    "    })\n",
    "\n",
    "# apply grid search to each model\n",
    "results = models_df.apply(train_model_with_gridsearch, axis=1)\n",
    "models_df = pd.concat([models_df, results], axis=1)\n",
    "\n",
    "# print results\n",
    "for _, row in models_df.iterrows():\n",
    "    print(f\"\\nModel: {row['name']}\")\n",
    "    print(f\"Best parameters: {row['best_params']}\")\n",
    "    print(f\"Best RMSE: {row['best_rmse']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>model</th>\n",
       "      <th>param_grid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear_regression</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>{'fit_intercept': [True, False], 'positive': [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>RandomForestRegressor(n_jobs=-1, random_state=...</td>\n",
       "      <td>{'n_estimators': [50, 100, 200], 'max_depth': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>XGBRegressor(base_score=None, booster=None, ca...</td>\n",
       "      <td>{'n_estimators': [50, 100, 200], 'max_depth': ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name                                              model  \\\n",
       "0  linear_regression                                 LinearRegression()   \n",
       "1      random_forest  RandomForestRegressor(n_jobs=-1, random_state=...   \n",
       "2            xgboost  XGBRegressor(base_score=None, booster=None, ca...   \n",
       "\n",
       "                                          param_grid  \n",
       "0  {'fit_intercept': [True, False], 'positive': [...  \n",
       "1  {'n_estimators': [50, 100, 200], 'max_depth': ...  \n",
       "2  {'n_estimators': [50, 100, 200], 'max_depth': ...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_df = pd.DataFrame([\n",
    "    {\n",
    "        'name': 'linear_regression',\n",
    "        'model': LinearRegression(),\n",
    "        'param_grid': {\n",
    "            'fit_intercept': [True, False],\n",
    "            'positive': [True, False]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'random_forest',\n",
    "        'model': RandomForestRegressor(random_state=1337, n_jobs=-1),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'xgboost',\n",
    "        'model': xgb.XGBRegressor(random_state=1337, n_jobs=-1),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [3, 6, 9],\n",
    "            'learning_rate': [0.01, 0.1, 0.3]\n",
    "        }\n",
    "    }\n",
    "])\n",
    "\n",
    "models_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lol :D Default linear regression is the best model so far :DDD\n",
    "\n",
    "I guess that's because the data set is tiny!\n",
    "\n",
    "Now make the function work for XGBoost as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it work for XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish Baseline: Linear Regression\n",
    "\n",
    "The final goal is to use a tree based model here.\n",
    "I want to use a simpler model as baseline though, and see if and how much a more\n",
    "sophistic model can improve over that.\n",
    "\n",
    "The way I prepared my data, also the label was normalized.\n",
    "I could have decided against that, and treat it as a classification task.\n",
    "I decided for it though to have all values in same range.\n",
    "The values are not discrete anymore, and this is now a regression task.\n",
    "\n",
    "The simplest model for this is linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a raw linear regression with default parameters to see how well it does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune parameters of linear regression to see how well it can get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train more sophisticated model: Decision Tree Regressor\n",
    "\n",
    "This is the classic tree based one. It has just one tree. It will be interesting\n",
    "to see if it will show improved performance over linear regression already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train more sophisticated model: Random Forest\n",
    "\n",
    "This the classic tree ensemble. It has more than one tree and may have improved\n",
    "performance in comparison to a single tree and the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train more sophisticated model: Extra Trees\n",
    "\n",
    "This is an alternative, but very interesting tree based ensemble.\n",
    "Unfortunately, I have never used it before, but I am highly interested in how it\n",
    "will perform.\n",
    "I heard the increased randomness is supposed to prevent overfitting, which may\n",
    "be beneficial for the tiny data set used here.\n",
    "The data set just has a few hundred rows (bit more if integration works), and\n",
    "the smaller the data set, the larger the risk of overfitting.\n",
    "Maybe XGBoost will already be overkill here, and Extra Trees will be just right.\n",
    "I am excited!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train more sophisticated model: XBGoost\n",
    "\n",
    "Frequently one of the best if not the best performing models in Kaggle\n",
    "competitions, and something like the crown of the evolution of the tree based\n",
    "models. Very powerful, but also prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RestAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
